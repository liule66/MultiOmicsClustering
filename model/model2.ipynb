{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T01:30:09.589830Z",
     "start_time": "2024-07-13T01:30:03.461062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入必要库\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "818f3dd27e345526",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T01:43:46.641407Z",
     "start_time": "2024-07-13T01:33:09.281543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据集函数\n",
    "def load_csv(file_path, nrows=None):\n",
    "    return pd.read_csv(file_path, sep='\\t', nrows=nrows, engine='python')\n",
    "\n",
    "# 加载数据集\n",
    "gene_expression = load_csv('EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena', nrows=20000)\n",
    "print(\"Size of gene_expression dataset: \", gene_expression.shape)\n",
    "dna_methylation = load_csv('DNA_methylation_450k', nrows=30000)\n",
    "print(\"Size of dna_methylation dataset: \", dna_methylation.shape)\n",
    "mirna_expression = load_csv('pancanMiRs_EBadjOnProtocolPlatformWithoutRepsWithUnCorrectMiRs_08_04_16.xena', nrows=700)\n",
    "print(\"Size of mirna_expression dataset: \", mirna_expression.shape)\n",
    "# 选取 clinical 数据\n",
    "clinical_raw_data = pd.read_csv('Survival_SupplementalTable_S1_20171025_xena_sp', sep='\\t', index_col=0)"
   ],
   "id": "34f273e444c81227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gene_expression dataset:  (20000, 11070)\n",
      "Size of dna_methylation dataset:  (30000, 9665)\n",
      "Size of mirna_expression dataset:  (700, 10825)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T02:29:04.013172Z",
     "start_time": "2024-07-13T02:29:02.635394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 获取每个数据集的列名（样本 ID）\n",
    "gene_expression_samples = set(gene_expression.columns)\n",
    "dna_methylation_samples = set(dna_methylation.columns)\n",
    "miRNA_expression_samples = set(mirna_expression.columns)\n",
    "clinical_samples = set(clinical_raw_data.index)\n",
    "\n",
    "# 找到所有数据集中共有的样本 ID\n",
    "common_samples = gene_expression_samples & dna_methylation_samples & miRNA_expression_samples & clinical_samples\n",
    "\n",
    "\n",
    "common_samples = list(common_samples)\n",
    "\n",
    "# 使用共有的样本 ID 来过滤每个数据集\n",
    "gene_expression_data = gene_expression[common_samples]\n",
    "dna_methylation_data = dna_methylation[common_samples]\n",
    "miRNA_expression_data = mirna_expression[common_samples]\n",
    "clinical_data = clinical_raw_data.loc[common_samples]\n",
    "print(\"Size of gene_expression_data: \", gene_expression_data.shape)\n",
    "print(\"Size of dna_methylation_data: \", dna_methylation_data.shape)\n",
    "print(\"Size of miRNA_expression_data: \", miRNA_expression_data.shape)\n",
    "\n",
    "print(\"load dataset Successfully\")"
   ],
   "id": "3df5a5febb6766e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gene_expression_data:  (20000, 8920)\n",
      "Size of dna_methylation_data:  (30000, 8920)\n",
      "Size of miRNA_expression_data:  (700, 8920)\n",
      "load dataset Successfully\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T02:32:37.334385Z",
     "start_time": "2024-07-13T02:30:51.492358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义清理函数\n",
    "\n",
    "def clean_data(data):\n",
    "    # 将非数字强制转换为NaN\n",
    "    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "    # 删除缺失值过多的列\n",
    "    data = data.dropna(axis=1, thresh=0.7*data.shape[0])\n",
    "    # 填充缺失值\n",
    "    data = data.apply(lambda row: row.fillna(row.mean()), axis=1)\n",
    "    # 异常值处理，这里使用简单的方法将所有值限制在其99%的分位数范围内\n",
    "    for column in data.columns:\n",
    "        upper_limit = data[column].quantile(0.99)\n",
    "        lower_limit = data[column].quantile(0.01)\n",
    "        data[column] = data[column].clip(lower=lower_limit, upper=upper_limit)\n",
    "    # 特征缩放\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(data), columns=data.columns)\n",
    "    return data\n",
    "\n",
    "# 清理每种数据\n",
    "gene_expression_data = clean_data(gene_expression_data)\n",
    "dna_methylation_data = clean_data(dna_methylation_data)\n",
    "mirna_expression_data = clean_data(miRNA_expression_data)\n",
    "# 将 clinical 数据中 OS.time 的NaN值替换为 last_contact_days_to 值\n",
    "clinical_data['OS.time'] = clinical_data['OS.time'].fillna(clinical_data['last_contact_days_to'])\n",
    "clinical_data['OS.time'].fillna(clinical_data['OS.time'].mean(), inplace=True)\n",
    "# 数据预处理\n",
    "# 只使用数值列进行标准化\n",
    "gene_expression_data = gene_expression_data.iloc[:, :].values.T\n",
    "dna_methylation_data = dna_methylation_data.iloc[:, :].values.T\n",
    "mirna_expression_data = mirna_expression_data.iloc[:, :].values.T\n",
    "\n",
    "print(\"Size of gene_expression_data: \", gene_expression_data.shape)\n",
    "print(\"Size of dna_methylation_data: \", dna_methylation_data.shape)\n",
    "print(\"Size of miRNA_expression_data: \", mirna_expression_data.shape)"
   ],
   "id": "fdd04ca36fdb2505",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gene_expression_data:  (8920, 20000)\n",
      "Size of dna_methylation_data:  (8920, 30000)\n",
      "Size of miRNA_expression_data:  (8920, 700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55635/4217385157.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clinical_data['OS.time'].fillna(clinical_data['OS.time'].mean(), inplace=True)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:07:24.263080Z",
     "start_time": "2024-07-13T03:03:19.281991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "gene_expression_scaled = scaler.fit_transform(gene_expression_data)\n",
    "dna_methylation_scaled = scaler.fit_transform(dna_methylation_data)\n",
    "mirna_expression_scaled = scaler.fit_transform(mirna_expression_data)\n",
    "\n",
    "# PCA降维\n",
    "pca_gene = PCA(n_components=1000)\n",
    "pca_dna = PCA(n_components=2000)\n",
    "pca_mirna = PCA(n_components=100)\n",
    "\n",
    "gene_expression_pca = pca_gene.fit_transform(gene_expression_scaled)\n",
    "dna_methylation_pca = pca_dna.fit_transform(dna_methylation_scaled)\n",
    "mirna_expression_pca = pca_mirna.fit_transform(mirna_expression_scaled)\n",
    "\n",
    "print(\"PCA step completed\")\n",
    "\n",
    "# 数据整合\n",
    "integrated_data = np.concatenate([gene_expression_pca, dna_methylation_pca, mirna_expression_pca], axis=1)\n",
    "print(\"Size of integrated_data: \", integrated_data.shape)\n",
    "\n",
    "# 转换为张量\n",
    "X_integrated = torch.tensor(integrated_data, dtype=torch.float32)\n",
    "y_dummy = torch.tensor(np.random.randint(0, 5, integrated_data.shape[0]), dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "dataset = TensorDataset(X_integrated,y_dummy)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "id": "86306960b5d9eca5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA step completed\n",
      "Size of integrated_data:  (8920, 3100)\n",
      "cuda\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:10:31.230436Z",
     "start_time": "2024-07-13T03:07:48.175843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义模型\n",
    "class ImprovedMultiOmicsNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedMultiOmicsNN, self).__init__()\n",
    "        self.fc_gene = nn.Sequential(\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 250),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(250, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_dna = nn.Sequential(\n",
    "            nn.Linear(2000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 320),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(320, 160),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(160, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mirna = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(32 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,3100)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_integrated):\n",
    "        x_gene_encoded = self.fc_gene(x_integrated[:, :1000])\n",
    "        x_dna_encoded = self.fc_dna(x_integrated[:, 1000:3000])\n",
    "        x_mirna_encoded = self.fc_mirna(x_integrated[:, 3000:])\n",
    "        x_combined = torch.cat((x_gene_encoded, x_dna_encoded, x_mirna_encoded), dim=1)\n",
    "        output = self.fc_combined(x_combined)\n",
    "        return output\n",
    "\n",
    "# 使用改进的模型\n",
    "model = ImprovedMultiOmicsNN().to(device)\n",
    "\n",
    "# 调整学习率和优化器\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# 定义重构损失函数\n",
    "def reconstruction_loss(reconstructed, original):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    return mse_loss(reconstructed, original)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 100\n",
    "prev_loss = float('inf')\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for X,_ in dataloader:\n",
    "\n",
    "        X = X.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs= model(X)\n",
    "        loss = reconstruction_loss(outputs, X)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    loss_list.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
   ],
   "id": "372c454cd6e4e115",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 10.0885\n",
      "Epoch [2/100], Loss: 7.5018\n",
      "Epoch [3/100], Loss: 6.9113\n",
      "Epoch [4/100], Loss: 6.6140\n",
      "Epoch [5/100], Loss: 6.4470\n",
      "Epoch [6/100], Loss: 6.3403\n",
      "Epoch [7/100], Loss: 6.2484\n",
      "Epoch [8/100], Loss: 6.2060\n",
      "Epoch [9/100], Loss: 6.1343\n",
      "Epoch [10/100], Loss: 6.1002\n",
      "Epoch [11/100], Loss: 6.0421\n",
      "Epoch [12/100], Loss: 6.0259\n",
      "Epoch [13/100], Loss: 5.9934\n",
      "Epoch [14/100], Loss: 5.9929\n",
      "Epoch [15/100], Loss: 5.9780\n",
      "Epoch [16/100], Loss: 5.9301\n",
      "Epoch [17/100], Loss: 5.9067\n",
      "Epoch [18/100], Loss: 5.9123\n",
      "Epoch [19/100], Loss: 5.8925\n",
      "Epoch [20/100], Loss: 5.8858\n",
      "Epoch [21/100], Loss: 5.8756\n",
      "Epoch [22/100], Loss: 5.8588\n",
      "Epoch [23/100], Loss: 5.8309\n",
      "Epoch [24/100], Loss: 5.8376\n",
      "Epoch [25/100], Loss: 5.9304\n",
      "Epoch [26/100], Loss: 5.8418\n",
      "Epoch [27/100], Loss: 5.7979\n",
      "Epoch [28/100], Loss: 5.7764\n",
      "Epoch [29/100], Loss: 5.7452\n",
      "Epoch [30/100], Loss: 5.7337\n",
      "Epoch [31/100], Loss: 5.7468\n",
      "Epoch [32/100], Loss: 5.7475\n",
      "Epoch [33/100], Loss: 5.7219\n",
      "Epoch [34/100], Loss: 5.7037\n",
      "Epoch [35/100], Loss: 5.6839\n",
      "Epoch [36/100], Loss: 5.7072\n",
      "Epoch [37/100], Loss: 5.7239\n",
      "Epoch [38/100], Loss: 5.6837\n",
      "Epoch [39/100], Loss: 5.6911\n",
      "Epoch [40/100], Loss: 5.6828\n",
      "Epoch [41/100], Loss: 5.6947\n",
      "Epoch [42/100], Loss: 5.6874\n",
      "Epoch [43/100], Loss: 5.6894\n",
      "Epoch [44/100], Loss: 5.6900\n",
      "Epoch [45/100], Loss: 5.7003\n",
      "Epoch [46/100], Loss: 5.6883\n",
      "Epoch [47/100], Loss: 5.7202\n",
      "Epoch [48/100], Loss: 5.6988\n",
      "Epoch [49/100], Loss: 5.6533\n",
      "Epoch [50/100], Loss: 5.6281\n",
      "Epoch [51/100], Loss: 5.6275\n",
      "Epoch [52/100], Loss: 5.6300\n",
      "Epoch [53/100], Loss: 5.6113\n",
      "Epoch [54/100], Loss: 5.6312\n",
      "Epoch [55/100], Loss: 5.6272\n",
      "Epoch [56/100], Loss: 5.6149\n",
      "Epoch [57/100], Loss: 5.6538\n",
      "Epoch [58/100], Loss: 5.6359\n",
      "Epoch [59/100], Loss: 5.6205\n",
      "Epoch [60/100], Loss: 5.6153\n",
      "Epoch [61/100], Loss: 5.6166\n",
      "Epoch [62/100], Loss: 5.6001\n",
      "Epoch [63/100], Loss: 5.5990\n",
      "Epoch [64/100], Loss: 5.6275\n",
      "Epoch [65/100], Loss: 5.6070\n",
      "Epoch [66/100], Loss: 5.5819\n",
      "Epoch [67/100], Loss: 5.5930\n",
      "Epoch [68/100], Loss: 5.6166\n",
      "Epoch [69/100], Loss: 5.6609\n",
      "Epoch [70/100], Loss: 5.5961\n",
      "Epoch [71/100], Loss: 5.5793\n",
      "Epoch [72/100], Loss: 5.5725\n",
      "Epoch [73/100], Loss: 5.5684\n",
      "Epoch [74/100], Loss: 5.5544\n",
      "Epoch [75/100], Loss: 5.6267\n",
      "Epoch [76/100], Loss: 5.6448\n",
      "Epoch [77/100], Loss: 5.6192\n",
      "Epoch [78/100], Loss: 5.5531\n",
      "Epoch [79/100], Loss: 5.5524\n",
      "Epoch [80/100], Loss: 5.5554\n",
      "Epoch [81/100], Loss: 5.5778\n",
      "Epoch [82/100], Loss: 5.5915\n",
      "Epoch [83/100], Loss: 5.5686\n",
      "Epoch [84/100], Loss: 5.6183\n",
      "Epoch [85/100], Loss: 5.5642\n",
      "Epoch [86/100], Loss: 5.5513\n",
      "Epoch [87/100], Loss: 5.5564\n",
      "Epoch [88/100], Loss: 5.5769\n",
      "Epoch [89/100], Loss: 5.5758\n",
      "Epoch [90/100], Loss: 5.5795\n",
      "Epoch [91/100], Loss: 5.5320\n",
      "Epoch [92/100], Loss: 5.5507\n",
      "Epoch [93/100], Loss: 5.5401\n",
      "Epoch [94/100], Loss: 5.5311\n",
      "Epoch [95/100], Loss: 5.5686\n",
      "Epoch [96/100], Loss: 5.5471\n",
      "Epoch [97/100], Loss: 5.5917\n",
      "Epoch [98/100], Loss: 5.5561\n",
      "Epoch [99/100], Loss: 5.5605\n",
      "Epoch [100/100], Loss: 5.5502\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:16:25.136333Z",
     "start_time": "2024-07-13T03:16:17.416827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 在整个数据集上重新计算模型的输出\n",
    "all_outputs = []\n",
    "model.eval()  # 切换到评估模式\n",
    "with torch.no_grad():\n",
    "    for X, _ in dataloader:\n",
    "        X = X.to(device)\n",
    "        output_batch = model(X)\n",
    "        all_outputs.append(output_batch.cpu().numpy())\n",
    "\n",
    "# 合并所有批次的输出\n",
    "all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "print(\"Size of all_outputs: \", all_outputs.shape)\n",
    "\n",
    "# 使用聚类算法进行聚类分析\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(all_outputs)\n",
    "\n",
    "# 结果评估（轮廓系数）\n",
    "silhouette_avg = silhouette_score(all_outputs, labels)\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n",
    "\n",
    "# 使用PCA降维\n",
    "pca = PCA(n_components=2)\n",
    "outputs_2d = pca.fit_transform(all_outputs)\n",
    "\n",
    "# 绘制散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(outputs_2d[:, 0], outputs_2d[:, 1], c=labels)\n",
    "plt.title('Cluster Visualization')\n",
    "plt.savefig('cluster_visualization.png')  # 保存图像而不是显示\n",
    "plt.close()\n",
    "\n",
    "# 绘制训练的损失曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_list, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('training_loss.png')  # 保存图像而不是显示\n",
    "plt.close()"
   ],
   "id": "3bd49c693859ae57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of all_outputs:  (8920, 3100)\n",
      "Silhouette Score: 0.13049563765525818\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-13T03:16:10.819149Z",
     "start_time": "2024-07-13T03:16:00.775437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_optimal_k_and_plot(survival_data, combined_data, k_range):\n",
    "    optimal_k = None\n",
    "    best_p_value = 1  # 初始化为最大的p值\n",
    "    best_labels = None\n",
    "    total_p_values = []\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(all_outputs)\n",
    "\n",
    "        survival_data['Cluster'] = labels\n",
    "        kmf = KaplanMeierFitter()\n",
    "        survival_curves = []\n",
    "\n",
    "        for cluster in np.unique(labels):\n",
    "            cluster_data = survival_data[survival_data['Cluster'] == cluster]\n",
    "            T = cluster_data['OS.time']\n",
    "            E = np.where(cluster_data['vital_status'] == 'Alive', 1, 0)\n",
    "            kmf.fit(T, event_observed=E)\n",
    "            survival_curves.append((T, E))\n",
    "\n",
    "        # 计算不同聚类之间的生存曲线差异\n",
    "        p_values = []\n",
    "        for i in range(len(survival_curves) - 1):\n",
    "            for j in range(i + 1, len(survival_curves)):\n",
    "                result = logrank_test(survival_curves[i][0], survival_curves[j][0],\n",
    "                                      event_observed_A=survival_curves[i][1], event_observed_B=survival_curves[j][1])\n",
    "                p_values.append(result.p_value)\n",
    "\n",
    "        # 选择差异最大（p值最小）的聚类结果\n",
    "        min_p_value = np.mean(p_values) if p_values else 1\n",
    "        total_p_values.append(min_p_value)\n",
    "        if min_p_value < best_p_value:\n",
    "            best_p_value = min_p_value\n",
    "            optimal_k = k\n",
    "            best_labels = labels\n",
    "\n",
    "    # 可视化不同簇数量下的p值\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, total_p_values, 'bx-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('P values')\n",
    "    plt.title('P values for Different k')\n",
    "    plt.savefig('p_values.png')  # 保存图像而不是显示\n",
    "    plt.close()\n",
    "\n",
    "    # 使用最优的k值进行聚类，并画出生存曲线图\n",
    "    if optimal_k:\n",
    "        print(f\"Optimal k: {optimal_k} with p-value: {best_p_value}\")\n",
    "        survival_data['Cluster'] = best_labels\n",
    "        kmf = KaplanMeierFitter()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for cluster in np.unique(best_labels):\n",
    "            cluster_data = survival_data[survival_data['Cluster'] == cluster]\n",
    "            T = cluster_data['OS.time']\n",
    "            E = np.where(cluster_data['vital_status'] == 'Alive', 1, 0)\n",
    "            kmf.fit(T, event_observed=E, label=f'Cluster {cluster}')\n",
    "            kmf.plot_survival_function()\n",
    "\n",
    "        plt.title('Survival Analysis by Cluster with Optimal k')\n",
    "        plt.savefig('survival_analysis.png')  # 保存图像而不是显示\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No optimal k found.\")\n",
    "\n",
    "find_optimal_k_and_plot(clinical_data, all_outputs, range(2, 10))"
   ],
   "id": "9fefe6f8e1734904",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal k: 3 with p-value: 0.40755662602297166\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaaeeedcb14faab3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
